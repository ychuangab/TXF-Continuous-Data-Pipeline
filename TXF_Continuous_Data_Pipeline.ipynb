{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TXF-Continuous-Data-Pipeline.ipynb\"\"\"\n",
        "\n",
        "# 安裝必要套件 (Colab 環境專用)\n",
        "!pip install shioaji\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gspread\n",
        "import shioaji as sj\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from typing import Tuple, Dict, Any, Optional\n",
        "\n",
        "# 嘗試載入 dotenv (用於本地端 .env 檔案)\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except ImportError:\n",
        "    pass  # 如果在 Colab 且沒安裝 python-dotenv，就跳過\n",
        "\n",
        "# ==========================================\n",
        "# 0. 環境偵測與變數載入器 (Environment Loader)\n",
        "# ==========================================\n",
        "\n",
        "def get_env_variable(var_name: str, required: bool = True) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    通用變數讀取器：\n",
        "    1. 優先讀取系統環境變數 (os.environ) -> 適用 Local/Docker/VPS\n",
        "    2. 其次讀取 Google Colab Userdata -> 適用 Colab\n",
        "    \"\"\"\n",
        "    # 1. Try System Environment Variable\n",
        "    value = os.getenv(var_name)\n",
        "    if value:\n",
        "        return value\n",
        "\n",
        "    # 2. Try Colab Userdata\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        return userdata.get(var_name)\n",
        "    except (ImportError, AttributeError, KeyError):\n",
        "        pass\n",
        "\n",
        "    # 3. Handle Missing Variable\n",
        "    if required:\n",
        "        raise EnvironmentError(f\"❌ 缺少必要環境變數: {var_name}\")\n",
        "    return None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN5I4D3BG5mL",
        "outputId": "71c13faf-5672-43e6-f45b-3f13a58c7b97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shioaji\n",
            "  Downloading shioaji-1.3.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting base58 (from shioaji)\n",
            "  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.4.1 in /usr/local/lib/python3.12/dist-packages (from shioaji) (3.20.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from shioaji) (8.7.1)\n",
            "Collecting loguru<=0.7.3,>=0.6.0 (from shioaji)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from shioaji) (2.12.3)\n",
            "Collecting pynacl>=1.3.0 (from shioaji)\n",
            "  Downloading pynacl-1.6.2-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting pyrsca (from shioaji)\n",
            "  Downloading pyrsca-0.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting pysolace>=0.9.53 (from shioaji)\n",
            "  Downloading pysolace-0.9.53-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from shioaji) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.5.12 in /usr/local/lib/python3.12/dist-packages (from shioaji) (2.49.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shioaji) (4.15.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from shioaji) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=1.0.0->shioaji) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=1.0.0->shioaji) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=1.0.0->shioaji) (0.4.2)\n",
            "Requirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pynacl>=1.3.0->shioaji) (2.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from pysolace>=0.9.53->shioaji) (1.1.2)\n",
            "Requirement already satisfied: orjson>=3.3.1 in /usr/local/lib/python3.12/dist-packages (from pysolace>=0.9.53->shioaji) (3.11.5)\n",
            "Requirement already satisfied: typer>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from pysolace>=0.9.53->shioaji) (0.21.1)\n",
            "Requirement already satisfied: urllib3>=1.26.11 in /usr/local/lib/python3.12/dist-packages (from sentry-sdk>=1.5.12->shioaji) (2.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from sentry-sdk>=1.5.12->shioaji) (2026.1.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->shioaji) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->shioaji) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->shioaji) (3.11)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->pynacl>=1.3.0->shioaji) (2.23)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.10.0->pysolace>=0.9.53->shioaji) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.10.0->pysolace>=0.9.53->shioaji) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.10.0->pysolace>=0.9.53->shioaji) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.10.0->pysolace>=0.9.53->shioaji) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.10.0->pysolace>=0.9.53->shioaji) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.10.0->pysolace>=0.9.53->shioaji) (0.1.2)\n",
            "Downloading shioaji-1.3.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (749 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.2/749.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynacl-1.6.2-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysolace-0.9.53-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Downloading pyrsca-0.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyrsca, loguru, base58, pynacl, pysolace, shioaji\n",
            "Successfully installed base58-2.1.1 loguru-0.7.3 pynacl-1.6.2 pyrsca-0.4.0 pysolace-0.9.53 shioaji-1.3.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2026-01-20 14:43:05.834\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mimportlib._bootstrap\u001b[0m:\u001b[36m_call_with_frames_removed\u001b[0m:\u001b[36m488\u001b[0m - \u001b[33m\u001b[1mOptional: pip install shioaji[speed] or uv add shioaji --extra speed for better performance.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. 全域配置與常數 (Configuration)\n",
        "# ==========================================\n",
        "\n",
        "print(\"=== Loading Configuration & Secrets ===\")\n",
        "\n",
        "# --- A. 功能開關 (Feature Flags) ---\n",
        "FORCE_MXFR1 = True      # 強制使用近月合約代碼 (MXFR1)\n",
        "TRIM_DATA = True        # 是否修剪非交易時段數據\n",
        "QUERY_BACK_DAYS = 7     # 回補天數 (往前抓幾天)\n",
        "\n",
        "# --- B. 連線重試設定 (Retry Config) ---\n",
        "RETRY_MAX = 3\n",
        "RETRY_DELAY_BASE = 1\n",
        "\n",
        "# --- C. 市場時段設定 (Market Hours) ---\n",
        "MARKET_HOURS = {\n",
        "    \"D\": {\"open\": \"08:45\", \"close\": \"13:45\"},\n",
        "    \"N\": {\"open\": \"15:00\", \"close\": \"05:00\"}\n",
        "}\n",
        "\n",
        "# --- D. Google Sheets 設定 (Tab Names) ---\n",
        "TAB_NAME_SETTLE = 'TXF_settle_date_price'\n",
        "TAB_NAME_5MIN   = '5mink_new'\n",
        "TAB_NAME_60MIN  = '60mink_1'\n",
        "\n",
        "# --- E. 機密資訊載入 (Secrets Loading) ---\n",
        "# 在這裡一次性讀取所有 Secrets，後續程式碼只使用這些常數\n",
        "try:\n",
        "    # API Keys\n",
        "    SHIOAJI_API_KEY = get_env_variable('SHIOAJI_API_KEY')\n",
        "    SHIOAJI_SECRET_KEY = get_env_variable('SHIOAJI_SECRET_KEY')\n",
        "\n",
        "    # Google Sheets IDs\n",
        "    GSHEET_ID_DATA = get_env_variable('GSHEET_ID_DATA')\n",
        "    GSHEET_ID_SETTLE = get_env_variable('GSHEET_ID_SETTLE')\n",
        "\n",
        "    # Google Credentials (JSON String)\n",
        "    GSHEET_CREDENTIALS_JSON = get_env_variable('GSHEET_CREDENTIALS')\n",
        "\n",
        "    print(\"✅ All secrets loaded successfully.\")\n",
        "\n",
        "except EnvironmentError as e:\n",
        "    print(e)\n",
        "    print(\"請檢查 .env 檔案 (Local) 或 Secrets 設定 (Colab)。\")\n",
        "    sys.exit(1) # 缺少變數直接停止程式"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl-yGFx7G_lY",
        "outputId": "c00d69cc-5762-462f-e127-0de5e83291b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Loading Configuration & Secrets ===\n",
            "✅ All secrets loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. 認證與連線工具 (AuthManager)\n",
        "# ==========================================\n",
        "\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "class AuthManager:\n",
        "    \"\"\"處理 Google Sheets 與 Shioaji 的連線\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_gsheet_client():\n",
        "        \"\"\"建立 Google Sheets 連線 (使用全域常數)\"\"\"\n",
        "        try:\n",
        "            if not GSHEET_CREDENTIALS_JSON:\n",
        "                raise ValueError(\"Credentials JSON is empty\")\n",
        "\n",
        "            creds_dict = json.loads(GSHEET_CREDENTIALS_JSON)\n",
        "            # 修正 private_key 的換行符號問題\n",
        "            creds_dict['private_key'] = creds_dict['private_key'].replace('\\\\n', '\\n')\n",
        "\n",
        "            creds = Credentials.from_service_account_info(\n",
        "                creds_dict,\n",
        "                scopes=['https://www.googleapis.com/auth/spreadsheets']\n",
        "            )\n",
        "            return gspread.authorize(creds)\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"Google Sheet Auth Failed: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_shioaji_api(max_retries=RETRY_MAX, base_delay=RETRY_DELAY_BASE):\n",
        "        \"\"\"建立 Shioaji API 連線 (使用全域常數)\"\"\"\n",
        "        api = sj.Shioaji()\n",
        "\n",
        "        print(\"[Auth] Logging into Shioaji...\")\n",
        "        try:\n",
        "            api.login(\n",
        "                api_key=SHIOAJI_API_KEY,      # 直接使用常數\n",
        "                secret_key=SHIOAJI_SECRET_KEY, # 直接使用常數\n",
        "                contracts_cb=lambda security_type: print(f\"{repr(security_type)} fetch done.\")\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"Shioaji Login Failed: {e}\")\n",
        "\n",
        "        # --- [Smart Retry] 檢查 API 用量 ---\n",
        "        for attempt in range(1, max_retries + 1):\n",
        "            try:\n",
        "                usage_bytes = api.usage()['bytes']\n",
        "                usage_mb = round(usage_bytes / (1024 * 1024), 2)\n",
        "                print(f\"[Auth] API Usage: {usage_mb} MB / 500 MB\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if attempt < max_retries:\n",
        "                    wait_time = base_delay * (2 ** (attempt - 1))\n",
        "                    print(f\"[Auth] 取得用量失敗 (第 {attempt} 次)，{wait_time} 秒後重試... 錯誤: {e}\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    print(f\"[Auth] Warning: 無法取得 API 用量 (已重試 {max_retries} 次)。\")\n",
        "\n",
        "        return api"
      ],
      "metadata": {
        "id": "Kdsgwv6OHB__"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. 核心邏輯：結算日計算 (SettleManager)\n",
        "# ==========================================\n",
        "\n",
        "class SettleManager:\n",
        "    \"\"\"處理結算日邏輯與合約代碼計算\"\"\"\n",
        "\n",
        "    def __init__(self, gc):\n",
        "        self.gc = gc\n",
        "        # 直接使用全域常數 GSHEET_ID_SETTLE\n",
        "        self.df_config = self._load_config()\n",
        "\n",
        "    def _load_config(self) -> pd.DataFrame:\n",
        "        \"\"\"讀取結算設定表\"\"\"\n",
        "        try:\n",
        "            worksheet = self.gc.open_by_key(GSHEET_ID_SETTLE).worksheet(TAB_NAME_SETTLE)\n",
        "            data = worksheet.get_all_values()\n",
        "            df = pd.DataFrame(data[1:], columns=data[0])\n",
        "\n",
        "            # 資料型態轉換\n",
        "            cols_to_numeric = ['next_contract_diff', 'accumulated_contract_diff']\n",
        "            for col in cols_to_numeric:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            cols_to_datetime = ['start_k', 'settle_k']\n",
        "            for col in cols_to_datetime:\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "            return df.dropna(subset=['contract_year_month'])\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error loading settle config: {e}\")\n",
        "\n",
        "    def calculate_next_contract(self) -> str:\n",
        "        \"\"\"計算下一個合約代碼 (MXF+YM)\"\"\"\n",
        "        last_row = self.df_config.iloc[-1]\n",
        "\n",
        "        # 1. 推算下個合約月份\n",
        "        last_ym_dt = datetime.strptime(str(last_row['contract_year_month']), '%Y%m')\n",
        "        new_ym_dt = last_ym_dt + timedelta(days=31)\n",
        "        new_contract_ym = new_ym_dt.strftime('%Y%m')\n",
        "\n",
        "        # 2. 推算下個結算日 (該月第三個週三)\n",
        "        first_day = datetime(new_ym_dt.year, new_ym_dt.month, 1)\n",
        "        third_wed = first_day + pd.DateOffset(weeks=2)\n",
        "        while third_wed.weekday() != 2:\n",
        "            third_wed += timedelta(days=1)\n",
        "\n",
        "        new_settle_k = third_wed + timedelta(hours=13, minutes=25)\n",
        "        new_start_k = last_row['settle_k'] + timedelta(minutes=5)\n",
        "\n",
        "        # 計算累積價差 (僅用於預測)\n",
        "        new_acc_diff = last_row['accumulated_contract_diff'] + last_row['next_contract_diff']\n",
        "\n",
        "        print(f\"[Info] Current Config End: {last_row['contract_year_month']}\")\n",
        "        print(f\"[Info] Predicted Next Contract: {new_contract_ym}, Settle: {new_settle_k}\")\n",
        "\n",
        "        # 將預測的新行加回記憶體中的 DataFrame\n",
        "        new_row = pd.DataFrame([{\n",
        "            'contract_year_month': new_contract_ym,\n",
        "            'accumulated_contract_diff': new_acc_diff,\n",
        "            'start_k': new_start_k,\n",
        "            'settle_k': new_settle_k\n",
        "        }])\n",
        "        self.df_config = pd.concat([self.df_config, new_row], ignore_index=True)\n",
        "\n",
        "        return 'MXF' + new_contract_ym"
      ],
      "metadata": {
        "id": "j7PBeQzLHGB1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. 核心邏輯：資料處理 (DataProcessor)\n",
        "# ==========================================\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"處理 K 棒資料的清洗、重取樣、價差調整與完整性檢查\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def fetch_and_parse_kbars(api, contract_code: str, days_back: int) -> pd.DataFrame:\n",
        "        \"\"\"從 Shioaji 抓取資料\"\"\"\n",
        "        now = datetime.now(timezone(timedelta(hours=+8)))\n",
        "        end_date = now.strftime(\"%Y-%m-%d\")\n",
        "        start_date = (now - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
        "\n",
        "        target_code = \"MXFR1\" if FORCE_MXFR1 else contract_code\n",
        "        if not FORCE_MXFR1 and not api.Contracts.Futures.MXF[target_code]:\n",
        "             target_code = \"MXFR1\"\n",
        "\n",
        "        print(f\"[Action] Fetching {target_code} from {start_date} to {end_date}...\")\n",
        "\n",
        "        contract = api.Contracts.Futures.MXF[target_code]\n",
        "        kbars = api.kbars(contract, start=start_date, end=end_date)\n",
        "\n",
        "        df = pd.DataFrame({**kbars}).drop(columns=[\"Amount\"])\n",
        "        if df.empty:\n",
        "            return df, target_code\n",
        "\n",
        "        df['ts'] = pd.to_datetime(df['ts'])\n",
        "        df = df.set_index(\"ts\").sort_index()\n",
        "\n",
        "        return df, target_code\n",
        "\n",
        "    @staticmethod\n",
        "    def resample_and_split(df_raw: pd.DataFrame, df_settle_config: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Resample, Split D/N, Back Adjust, Add Metadata\"\"\"\n",
        "\n",
        "        # 1. 轉 5分K\n",
        "        df_5m = df_raw.resample('5min', label=\"left\", closed='right').agg({\n",
        "            'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last', 'Volume': 'sum'\n",
        "        }).dropna()\n",
        "\n",
        "        # 2. 分切日盤/夜盤\n",
        "        df_5m_D = df_5m.between_time(MARKET_HOURS[\"D\"][\"open\"], MARKET_HOURS[\"D\"][\"close\"]).copy()\n",
        "        df_5m_N = df_5m.between_time(MARKET_HOURS[\"N\"][\"open\"], MARKET_HOURS[\"N\"][\"close\"]).copy()\n",
        "\n",
        "        # 3. 轉 60分K\n",
        "        df_60m_D = df_5m_D.resample(\"60min\", offset=\"45min\").agg({\n",
        "            'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last', 'Volume': 'sum'\n",
        "        }).dropna()\n",
        "\n",
        "        df_60m_N = df_5m_N.resample(\"60min\").agg({\n",
        "            'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last', 'Volume': 'sum'\n",
        "        }).dropna()\n",
        "\n",
        "        # --- 補回 date_market_type (依據盤別開始日) ---\n",
        "        def get_market_date_str(ts, is_night=False):\n",
        "            target_ts = ts\n",
        "            # 只有在凌晨 (00:00-05:00) 且是夜盤時，才需要減一天\n",
        "            if is_night and ts.hour < 5:\n",
        "                target_ts = ts - timedelta(days=1)\n",
        "            suffix = \"N\" if is_night else \"D\"\n",
        "            return target_ts.strftime(\"%y%m%d\") + suffix\n",
        "\n",
        "        for df_temp, is_night in [(df_5m_D, False), (df_5m_N, True), (df_60m_D, False), (df_60m_N, True)]:\n",
        "             if not df_temp.empty:\n",
        "                 df_temp['date_market_type'] = df_temp.index.to_series().apply(lambda x: get_market_date_str(x, is_night))\n",
        "\n",
        "        # 4. 價差調整與欄位補全\n",
        "        def process_final_df(df_d, df_n):\n",
        "            df_all = pd.concat([df_d, df_n]).sort_index()\n",
        "            if df_all.empty: return df_all\n",
        "\n",
        "            df_all['contract_year_month'] = \"\"\n",
        "            df_all['accumulated_contract_diff'] = 0\n",
        "\n",
        "            def enrich_row(row):\n",
        "                match = df_settle_config[\n",
        "                    (row.name >= df_settle_config['start_k']) &\n",
        "                    (row.name <= df_settle_config['settle_k'])\n",
        "                ]\n",
        "                res = row.copy()\n",
        "                if not match.empty:\n",
        "                    cfg = match.iloc[0]\n",
        "                    diff = int(cfg['accumulated_contract_diff'])\n",
        "                    res['accumulated_contract_diff'] = diff\n",
        "                    res['contract_year_month'] = cfg['contract_year_month']\n",
        "                    res['Open'] += diff\n",
        "                    res['High'] += diff\n",
        "                    res['Low'] += diff\n",
        "                    res['Close'] += diff\n",
        "                return res\n",
        "\n",
        "            return df_all.apply(enrich_row, axis=1)\n",
        "\n",
        "        df_5m_final = process_final_df(df_5m_D, df_5m_N)\n",
        "        df_60m_final = process_final_df(df_60m_D, df_60m_N)\n",
        "\n",
        "        return df_5m_final, df_60m_final\n",
        "\n",
        "    @staticmethod\n",
        "    def drop_incomplete_current_session(df: pd.DataFrame, timeframe: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        [過濾器] 若當下的盤別尚未收盤 (資料筆數不足)，直接捨棄該盤所有資料，不進行上傳。\n",
        "        確保上傳的都是「已完結」的盤。\n",
        "        \"\"\"\n",
        "        EXPECTED = {\n",
        "            '5min':  {'D': 60, 'N': 168},\n",
        "            '60min': {'D': 5,  'N': 14}\n",
        "        }\n",
        "        if timeframe not in EXPECTED or df.empty: return df\n",
        "\n",
        "        # 1. 取得最後一筆資料的盤別分組\n",
        "        last_ts = df.index[-1]\n",
        "\n",
        "        # 定義分組邏輯 (Local function to avoid repetition)\n",
        "        def get_group_id(ts):\n",
        "            if 8 <= ts.hour <= 13: return ts.strftime('%Y-%m-%d') + '_D'\n",
        "            elif ts.hour >= 15: return ts.strftime('%Y-%m-%d') + '_N'\n",
        "            elif ts.hour < 5: return (ts - timedelta(days=1)).strftime('%Y-%m-%d') + '_N'\n",
        "            return 'UNKNOWN'\n",
        "\n",
        "        last_group_id = get_group_id(last_ts)\n",
        "\n",
        "        # 2. 判斷當下時間是否就是該盤 (是否正在進行中)\n",
        "        now = datetime.now(timezone(timedelta(hours=+8)))\n",
        "        current_active_id = get_group_id(now)\n",
        "\n",
        "        # 3. 檢查筆數\n",
        "        # 為了效能，只抓最後 200 筆來判斷即可\n",
        "        df_tail = df.iloc[-200:].copy()\n",
        "        df_tail['group'] = df_tail.index.to_series().apply(get_group_id)\n",
        "\n",
        "        last_group_count = len(df_tail[df_tail['group'] == last_group_id])\n",
        "        expected_count = EXPECTED[timeframe].get(last_group_id.split('_')[-1], 0)\n",
        "\n",
        "        # 4. 決策：如果是「正在進行中」且「筆數不足」，則丟棄\n",
        "        if last_group_id == current_active_id and last_group_count < expected_count:\n",
        "            print(f\"[Filter] 偵測到盤中資料 {last_group_id} 尚未收盤 ({last_group_count}/{expected_count}) -> 捨棄不處理 (寧缺勿濫)。\")\n",
        "            return df.iloc[:-last_group_count]\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def check_completeness(df: pd.DataFrame, timeframe: str):\n",
        "        \"\"\"[資料完整性檢查] (嚴格版：所有資料必須完整)\"\"\"\n",
        "        EXPECTED = {\n",
        "            '5min':  {'D': 60, 'N': 168},\n",
        "            '60min': {'D': 5,  'N': 14}\n",
        "        }\n",
        "        if timeframe not in EXPECTED or df.empty: return\n",
        "\n",
        "        print(f\"[Check] Verifying data completeness for {timeframe}...\")\n",
        "        ts_series = df.index.to_series()\n",
        "\n",
        "        def get_group_id(ts):\n",
        "            if 8 <= ts.hour <= 13: return ts.strftime('%Y-%m-%d') + '_D'\n",
        "            elif ts.hour >= 15: return ts.strftime('%Y-%m-%d') + '_N'\n",
        "            elif ts.hour < 5:\n",
        "                prev_date = ts - timedelta(days=1)\n",
        "                return prev_date.strftime('%Y-%m-%d') + '_N'\n",
        "            return 'UNKNOWN'\n",
        "\n",
        "        groups = ts_series.apply(get_group_id)\n",
        "        counts = groups.value_counts()\n",
        "        errors = []\n",
        "\n",
        "        for group_id, count in counts.items():\n",
        "            if 'UNKNOWN' in group_id: continue\n",
        "            market_type = group_id.split('_')[-1]\n",
        "            expected_count = EXPECTED[timeframe][market_type]\n",
        "            if count != expected_count:\n",
        "                errors.append(f\"  - {group_id}: 預期 {expected_count} 筆, 實際 {count} 筆\")\n",
        "\n",
        "        if errors:\n",
        "            raise ValueError(f\"資料完整性檢查失敗 ({timeframe})，停止上傳！\\n\" + \"\\n\".join(errors))\n",
        "        print(f\"[Check] {timeframe} Pass. All sessions appear complete.\")"
      ],
      "metadata": {
        "id": "Onhs1vN0HJ8v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. 上傳工具 (SheetUploader)\n",
        "# ==========================================\n",
        "\n",
        "class SheetUploader:\n",
        "    \"\"\"處理 Google Sheets 的寫入與上傳邏輯\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_last_timestamp(gc, tab_name: str) -> Optional[pd.Timestamp]:\n",
        "        \"\"\"[新增] 讀取 Google Sheet 上最後一筆資料的時間\"\"\"\n",
        "        try:\n",
        "            worksheet = gc.open_by_key(GSHEET_ID_DATA).worksheet(tab_name)\n",
        "            # 為了效能，我們只抓最後幾列來判斷即可，不需要抓整張表\n",
        "            # 但考慮到可能表是空的，我們用安全的方法\n",
        "            all_values = worksheet.get_all_values()\n",
        "\n",
        "            if len(all_values) < 2: # 只有標頭或空的\n",
        "                return None\n",
        "\n",
        "            # 假設第一欄是 ts (根據 _prepare_data 的邏輯)\n",
        "            headers = all_values[0]\n",
        "            if 'ts' not in headers:\n",
        "                return None\n",
        "\n",
        "            ts_idx = headers.index('ts')\n",
        "            last_row = all_values[-1]\n",
        "            last_ts_str = last_row[ts_idx]\n",
        "\n",
        "            return pd.to_datetime(last_ts_str)\n",
        "\n",
        "        except (gspread.WorksheetNotFound, Exception) as e:\n",
        "            # 如果表不存在，代表全是新資料，回傳 None\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _prepare_data(df_new: pd.DataFrame, existing_data: list) -> Tuple[pd.DataFrame, bool]:\n",
        "        \"\"\"[純邏輯] 資料清洗與比對\"\"\"\n",
        "        df_process = df_new.copy()\n",
        "        if df_process.index.name == 'ts':\n",
        "             df_process.reset_index(inplace=True)\n",
        "\n",
        "        # 確保 ts 是字串格式以便上傳\n",
        "        if 'ts' in df_process.columns:\n",
        "             df_process['ts'] = pd.to_datetime(df_process['ts']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # A. 空表\n",
        "        if not existing_data:\n",
        "            cols = ['ts'] + [c for c in df_process.columns if c != 'ts']\n",
        "            return df_process[cols], True\n",
        "\n",
        "        # B. 只有表頭\n",
        "        headers = existing_data[0]\n",
        "        if len(existing_data) == 1:\n",
        "            valid_cols = [c for c in headers if c in df_process.columns]\n",
        "            return df_process[valid_cols], False\n",
        "\n",
        "        # C. 正常更新 (這裡其實已經在 Main 做過過濾了，但保留著當雙重保險)\n",
        "        try:\n",
        "            ts_col_idx = headers.index('ts') if 'ts' in headers else 0\n",
        "            last_ts_str = existing_data[-1][ts_col_idx]\n",
        "            last_ts = pd.to_datetime(last_ts_str)\n",
        "\n",
        "            current_ts_series = pd.to_datetime(df_process['ts'])\n",
        "            df_to_upload = df_process[current_ts_series > last_ts].copy()\n",
        "\n",
        "            if df_to_upload.empty:\n",
        "                return pd.DataFrame(), False\n",
        "\n",
        "            valid_cols = [c for c in headers if c in df_to_upload.columns]\n",
        "            return df_to_upload[valid_cols], False\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] Data preparation failed: {e}\")\n",
        "            return pd.DataFrame(), False\n",
        "\n",
        "    @staticmethod\n",
        "    def append_safely(gc, tab_name: str, df_new: pd.DataFrame):\n",
        "        \"\"\"[I/O 操作] 連線並執行上傳 (使用全域常數 GSHEET_ID_DATA)\"\"\"\n",
        "        if df_new.empty:\n",
        "            print(f\"[{tab_name}] No new data to upload (Filter blocked).\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            worksheet = gc.open_by_key(GSHEET_ID_DATA).worksheet(tab_name)\n",
        "            existing_data = worksheet.get_all_values()\n",
        "        except gspread.WorksheetNotFound:\n",
        "            print(f\"[{tab_name}] Worksheet not found. Creating new...\")\n",
        "            # 這裡簡單處理：如果找不到就當作空表，但通常應該要先手動建好\n",
        "            existing_data = []\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] Google Sheet Connection failed: {e}\")\n",
        "            return\n",
        "\n",
        "        df_export, needs_header = SheetUploader._prepare_data(df_new, existing_data)\n",
        "\n",
        "        if not df_export.empty:\n",
        "            print(f\"[{tab_name}] Uploading {len(df_export)} rows...\")\n",
        "            data_to_write = df_export.values.tolist()\n",
        "            if needs_header:\n",
        "                print(f\"[{tab_name}] Detect empty sheet. Writing headers first.\")\n",
        "                data_to_write = [df_export.columns.tolist()] + data_to_write\n",
        "\n",
        "            try:\n",
        "                worksheet.append_rows(data_to_write)\n",
        "                print(f\"[{tab_name}] Upload Success.\")\n",
        "            except Exception as e:\n",
        "                print(f\"[{tab_name}] Upload Failed: {e}\")\n",
        "        else:\n",
        "            print(f\"[{tab_name}] No new data to upload.\")"
      ],
      "metadata": {
        "id": "6r-ShKGLHMqU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. 主程式 (Main Execution)\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        print(\"=== Automation Started ===\")\n",
        "\n",
        "        # 1. 初始化\n",
        "        gc = AuthManager.get_gsheet_client()\n",
        "        settle_mgr = SettleManager(gc)\n",
        "\n",
        "        # 2. 取得合約\n",
        "        target_contract = settle_mgr.calculate_next_contract()\n",
        "\n",
        "        # 3. API 抓取\n",
        "        api = AuthManager.get_shioaji_api()\n",
        "        df_raw, used_code = DataProcessor.fetch_and_parse_kbars(api, target_contract, QUERY_BACK_DAYS)\n",
        "        api.logout()\n",
        "\n",
        "        if not df_raw.empty:\n",
        "            # 4. 數據處理 (Resample -> D/N Split -> Back Adjust)\n",
        "            df_5m, df_60m = DataProcessor.resample_and_split(df_raw, settle_mgr.df_config)\n",
        "\n",
        "            # 4.1 過濾掉尚未收盤的當下資料 (避免盤中資料不全導致報錯)\n",
        "            df_5m = DataProcessor.drop_incomplete_current_session(df_5m, '5min')\n",
        "            df_60m = DataProcessor.drop_incomplete_current_session(df_60m, '60min')\n",
        "\n",
        "            # ================================================================\n",
        "            # 4.2 [關鍵修正] 增量更新過濾 (Incremental Filter)\n",
        "            # 先去 Sheet 看最後一筆是幾點，只保留「比它新」的資料\n",
        "            # 這樣舊日期的資料缺漏 (如 01-12) 就會被這裡濾掉，不會觸發 Error\n",
        "            # ================================================================\n",
        "\n",
        "            def filter_new_only(df, tab_name):\n",
        "                last_ts = SheetUploader.get_last_timestamp(gc, tab_name)\n",
        "                if last_ts is not None and not df.empty:\n",
        "                    original_count = len(df)\n",
        "                    # 保留 index 時間大於 Sheet 最後時間的資料\n",
        "                    df_new = df[df.index > last_ts].copy()\n",
        "                    filtered_count = len(df_new)\n",
        "                    if filtered_count < original_count:\n",
        "                        print(f\"[{tab_name}] Incremental Filter: {original_count} -> {filtered_count} rows (Dropped old data).\")\n",
        "                    return df_new\n",
        "                return df\n",
        "\n",
        "            print(\"--- Filtering New Data Only ---\")\n",
        "            df_5m = filter_new_only(df_5m, TAB_NAME_5MIN)\n",
        "            df_60m = filter_new_only(df_60m, TAB_NAME_60MIN)\n",
        "\n",
        "            # 5. 完整性檢查\n",
        "            # (現在這裡只會檢查「真正要上傳」的新資料，舊的壞資料已經被濾掉了)\n",
        "            if not df_5m.empty:\n",
        "                DataProcessor.check_completeness(df_5m, '5min')\n",
        "                # 補上代碼\n",
        "                df_5m['MXF_code'] = used_code\n",
        "                # 6. 上傳\n",
        "                SheetUploader.append_safely(gc, TAB_NAME_5MIN, df_5m)\n",
        "            else:\n",
        "                print(f\"[{TAB_NAME_5MIN}] All data is up-to-date. Skipping check & upload.\")\n",
        "\n",
        "            if not df_60m.empty:\n",
        "                DataProcessor.check_completeness(df_60m, '60min')\n",
        "                # 補上代碼\n",
        "                df_60m['MXF_code'] = used_code\n",
        "                # 6. 上傳\n",
        "                SheetUploader.append_safely(gc, TAB_NAME_60MIN, df_60m)\n",
        "            else:\n",
        "                print(f\"[{TAB_NAME_60MIN}] All data is up-to-date. Skipping check & upload.\")\n",
        "\n",
        "        else:\n",
        "            print(\"[Warning] No data fetched from API.\")\n",
        "\n",
        "        print(f\"=== Automation Finished in {round(time.time() - start_time, 2)}s ===\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"\\n[DATA INTEGRITY ERROR] -------------------------\")\n",
        "        print(ve)\n",
        "        print(f\"------------------------------------------------\")\n",
        "        print(\"上傳已終止，請檢查源頭資料狀況。\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[FATAL ERROR] Script crashed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpjjitFgHVQQ",
        "outputId": "98266587-412c-4849-8b2f-1205ff2dc2e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Automation Started ===\n",
            "[Info] Current Config End: 202512\n",
            "[Info] Predicted Next Contract: 202601, Settle: 2026-01-21 13:25:00\n",
            "[Auth] Logging into Shioaji...\n",
            "Response Code: 0 | Event Code: 0 | Info: host '210.59.255.161:80', hostname '210.59.255.161:80' IP 210.59.255.161:80 (host 1 of 1) (host connection attempt 1 of 1) (total connection attempt 1 of 1) | Event: Session up\n",
            "<SecurityType.Index: 'IND'> fetch done.\n",
            "Response Code: 200 | Event Code: 16 | Info: APISUB/V1/SYS/CONTRACT | Event: Subscribe or Unsubscribe ok\n",
            "[Auth] API Usage: 15.83 MB / 500 MB\n",
            "[Action] Fetching MXFR1 from 2026-01-13 to 2026-01-20...\n",
            "<SecurityType.Future: 'FUT'> fetch done.\n",
            "<SecurityType.Option: 'OPT'> fetch done.\n",
            "<SecurityType.Stock: 'STK'> fetch done.\n",
            "[Filter] 偵測到盤中資料 2026-01-20_N 尚未收盤 (93/168) -> 捨棄不處理 (寧缺勿濫)。\n",
            "[Filter] 偵測到盤中資料 2026-01-20_N 尚未收盤 (8/14) -> 捨棄不處理 (寧缺勿濫)。\n",
            "--- Filtering New Data Only ---\n",
            "[5mink_new] Incremental Filter: 1261 -> 228 rows (Dropped old data).\n",
            "[60mink_1] Incremental Filter: 106 -> 19 rows (Dropped old data).\n",
            "[Check] Verifying data completeness for 5min...\n",
            "[Check] 5min Pass. All sessions appear complete.\n",
            "[5mink_new] Uploading 228 rows...\n",
            "[5mink_new] Upload Success.\n",
            "[Check] Verifying data completeness for 60min...\n",
            "[Check] 60min Pass. All sessions appear complete.\n",
            "[60mink_1] Uploading 19 rows...\n",
            "[60mink_1] Upload Success.\n",
            "=== Automation Finished in 27.48s ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzWlHpxecC7Q"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}